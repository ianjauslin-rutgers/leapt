#!/usr/bin/env python3
import openai
import sys
import os
import subprocess
import re
import datetime
import configparser
import argparse

## defaults
# version of gpt to use
gpt_ver="gpt-3.5-turbo"

# path to write lean code to
lean_path=os.path.dirname(sys.argv[0])+'/gpt.lean'
# API key stored at
api_key_path=os.path.dirname(sys.argv[0])+'/openai.key'
# configuration file
config_file=os.path.dirname(sys.argv[0])+'/leapt.conf'
# tmpfile for prompt editing
tmp_prompt="/tmp/leapt_prompt.txt"
# editor for prompt editing
editor="nano"
# save history to file
history_file=os.path.dirname(sys.argv[0])+'/history/'+datetime.datetime.now().strftime("%Y-%m-%d_%H-%M-%S")

# first prompt
first_prompt="This is a test."

# instructions passed to gpt
gpt_instructions="Be a helpful assistant"

# maximum number of queries
max_queries=100
# maximum number of tokens
max_tokens=10000
# be interactive every n queries
interactive_freq=1

# colors
# header format
fmt_header='\033[1;32m'
# >>>> format 
fmt_lead='\033[1;32m'
# token usage format 
fmt_token='\033[1;31m'
# error format 
fmt_error='\033[1;35m'
# question format 
fmt_question='\033[1;36m'
# normal
fmt_normal='\033[0;39m'


# init token count
completion_token_count=0
prompt_token_count=0


# CLI arguments
def parse_CLI():
    parser=argparse.ArgumentParser(description="leapt: An attempt to get GPT to generate formal lean proofs")

    parser.add_argument("-c", metavar="conf_file", help="path to configuration file")

    args=parser.parse_args()
    if args.c!=None:
        global config_file
        config_file=args.c


# read config file
def parse_config(file):
    config=configparser.ConfigParser()
    config.read(file)

    if "gpt" in config:
        if "first_prompt" in config["gpt"]:
            global first_prompt
            first_prompt=config["gpt"]["first_prompt"]
        if "gpt_instructions" in config["gpt"]:
            global gpt_instructions
            gpt_instructions=config["gpt"]["gpt_instructions"]
    if "openai" in config:
        if "gpt_ver" in config["openai"]:
            global gpt_ver
            gpt_ver=config["openai"]["gpt_ver"]
        if "api_key" in config["openai"]:
            openai.api_key=config["openai"]["api_key"]
    if "limits" in config:
        if "max_queries" in config["limits"]:
            global max_queries
            max_queries=int(config["limits"]["max_queries"])
        if "max_tokens" in config["limits"]:
            global max_tokens
            max_tokens=int(config["limits"]["max_tokens"])
        if "interactive_freq" in config["limits"]:
            global interactive_freq
            interactive_freq=int(config["limits"]["interactive_freq"])
    if "general" in config:
        if "editor" in config["general"]:
            global editor
            editor=config["general"]["editor"]
        if "lean_path" in config["general"]:
            global lean_path
            lean_path=relpath(config["general"]["lean_path"])
        if "api_key_path" in config["general"]:
            global api_key_path
            api_key_path=relpath(config["general"]["api_key_path"])
        if "tmp_prompt" in config["general"]:
            global tmp_prompt
            tmp_prompt=relpath(config["general"]["tmp_prompt"])
        if "history_file" in config["general"]:
            global history_file
            history_file=relpath(config["general"]["history_file"])

# relative path to executable directory
def relpath(path):
    if len(path)==0 or path[0]!='/':
        return os.path.dirname(sys.argv[0])+'/'+path
    else:
        return path

# read key from file
def read_api_key():
    # check it exists
    if not os.path.exists(api_key_path):
        print("Error: could not find API key at '"+api_key_path+"'",file=sys.stderr)
        exit(-1)
    api_key_file=open(api_key_path,'r')
    openai.api_key=api_key_file.read().rstrip()
    api_key_file.close()

# query GPT
def GPT_query(prompt, history, gpt_ver):

    # write prompt to history
    history.append(
        {"role": "user", "content": prompt},
    )

    if gpt_ver=="gpt-4":
        response=openai.ChatCompletion.create(
          model="gpt-4",
          messages=history
        )
        out=response["choices"][0]["text"]
        completion_tokens=response["usage"]["completion_tokens"]
        prompt_tokens=response["usage"]["completion_tokens"]
    elif gpt_ver=="gpt-3.5-turbo":
        response=openai.ChatCompletion.create(
          model="gpt-3.5-turbo",
          messages=history
        )
        out=response["choices"][0]["message"]["content"]
        completion_tokens=response["usage"]["completion_tokens"]
        prompt_tokens=response["usage"]["completion_tokens"]
    elif gpt_ver=="test":
        out="```\n theorem even_square_implies_even (n : ℤ) : n^2 % 2 = 0 → n % 2 = 0 :=\n begin\n intro h,\n cases int.mod_two_eq_zero_or_one n with hn1 hn2,\n { exact hn2 },\n { exfalso,\n have h2 : n^2 % 2 = 1,\n { rw [int.sq, hn1], norm_num },\n rw h at h2,\n norm_num at h2, },\n end\n ```"
        completion_tokens=0
        prompt_tokens=0
    else:
        print("error: unspported gpt version: '"+gpt_ver+"'", file=sys.stderr)
        exit(-1)

    print("used "+fmt_token+str(completion_tokens)+" completion tokens, "+str(prompt_tokens)+" prompt tokens (total "+str(completion_tokens+prompt_tokens)+")"+fmt_normal, file=sys.stderr)

    # count tokens
    global completion_token_count
    global prompt_token_count
    completion_token_count=completion_token_count+completion_tokens
    prompt_token_count=prompt_token_count+prompt_tokens

    # add response to history
    history.append(
        {"role": "assistant", "content": out}
    )
    return out

# extract lean code from GPT response
def lean_extract(text):
    # default
    out=text
    # get code in between ``` if available
    matches=re.finditer(r'```(lean)?(.*?)```', text, flags=re.DOTALL)
    for match in matches:
        out=match.group(2)
    return out

# run lean on code in 'text'
def lean_run(text):
    # write to file
    outfile=open(lean_path,'w')
    print(text, file=outfile)
    outfile.close()

    # run lean
    res = subprocess.run(['lean', lean_path], stdout=subprocess.PIPE)

    # output
    return res.stdout.decode("utf-8")

# edit a string in editor
def edit(text, path):
    # write prompt
    tmpfile=open(path, 'w')
    print(text, file=tmpfile)
    tmpfile.close()
    # edit prompt
    subprocess.run([editor, path])
    # read prompt
    tmpfile=open(path, 'r')
    out=tmpfile.read()
    tmpfile.close()
    return out

# read CLI and configuration file
parse_config(config_file)
parse_CLI()

# read api key if not already in config
if openai.api_key == None:
    read_api_key()

# init history
history=[
    # this line tells the model how to behave
    {"role": "system", "content": gpt_instructions},
]

# first prompt
prompt=first_prompt

# loop
counter=1
while (max_queries==0 or counter<max_queries) and (max_tokens==0 or completion_token_count+prompt_token_count<max_tokens):
    print(fmt_lead+">>>>"+fmt_normal+" total usage so far: "+fmt_token+str(completion_token_count)+" completion tokens, "+str(prompt_token_count)+" prompt tokens (total "+str(completion_token_count+prompt_token_count)+")"+fmt_normal, file=sys.stderr)

    print(fmt_lead+">>>>"+fmt_header+" Prompt"+fmt_normal, file=sys.stderr)
    print(prompt, file=sys.stdout)

    # interactive
    cont=True
    if interactive_freq>0 and counter%interactive_freq==0:
        while True:
            print(fmt_question+"Continue? [Y/n/edit] "+fmt_normal, file=sys.stderr, end="")
            reply=input().rstrip()
            if reply=="" or reply=="Y" or reply=="y" or reply=="Yes" or reply=="yes":
                cont=True
                break
            elif reply=="n" or reply=="N" or reply=="no" or reply=="No" or reply=="q" or reply=="quit":
                cont=False
                break
            # open an editor to edit prompt
            elif reply=="edit":
                prompt=edit(prompt, tmp_prompt)
                # print for confirmation
                print("", file=sys.stderr)
                print(fmt_lead+">>>>"+fmt_header+" Prompt:"+fmt_normal, file=sys.stderr)
                print(prompt, file=sys.stderr)
                continue
            else:
                print(fmt_error+"Did not understand input"+fmt_normal, file=sys.stderr)
                continue

    # stop if no
    if cont==False:
        break

    # send to GPT
    gpt_out=GPT_query(prompt, history, gpt_ver)

    # print
    print(fmt_lead+">>>>"+fmt_header+" Response"+fmt_normal, file=sys.stderr)
    print(gpt_out, file=sys.stdout)
    print("", file=sys.stderr)

    to_lean=lean_extract(gpt_out)
    print(fmt_lead+">>>>"+fmt_header+" Pass to lean"+fmt_normal, file=sys.stderr)
    print(to_lean, file=sys.stdout)
    print("", file=sys.stderr)

    # interactive
    if interactive_freq>0 and counter%interactive_freq==0:
        while True:
            print(fmt_question+"Edit? [N/y] "+fmt_normal, file=sys.stderr, end="")
            reply=input().rstrip()
            if reply=="" or reply=="n" or reply=="N" or reply=="no" or reply=="No":
                break
            # open an editor to edit prompt
            elif reply=="y" or reply=="Y" or reply=="yes" or reply=="Yes" or reply=="edit":
                to_lean=edit(to_lean, tmp_prompt)
                # print for confirmation
                print("", file=sys.stderr)
                print(fmt_lead+">>>>"+fmt_header+" Pass to lean:"+fmt_normal, file=sys.stderr)
                print(to_lean, file=sys.stderr)
                continue
            else:
                print(fmt_error+"Did not understand input"+fmt_normal, file=sys.stderr)
                continue

    # output from lean
    lean_output=lean_run(to_lean)
    # filter lean output (remove path to file)
    lean_output=re.sub(r'^/[^:]*:',"", lean_output, flags=re.MULTILINE)

    # print
    print(fmt_lead+">>>>"+fmt_header+" Lean returns"+fmt_normal, file=sys.stderr)
    print(lean_output, file=sys.stdout)
    print("", file=sys.stderr)

    # success?
    if len(lean_output)==0:
        break

    # prepare prompt for next iteration
    prompt="This failed with the error: \""+lean_output+"\""

    # increment counter
    counter=counter+1

# print chat to file
outfile=open(history_file, 'w')
print(history, file=outfile)
outfile.close()
